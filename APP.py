
import streamlit as st
from streamlit_option_menu import option_menu
from openai import OpenAI
import tempfile
import base64
import os
import json
import math
import io
from pydub import AudioSegment

# åˆå§‹åŒ– OpenAI
API_KEY = st.secrets["openai_key"]
client = OpenAI(api_key=API_KEY)

# è¼‰å…¥ prompt
with open("prompt.json", "r", encoding="utf-8") as f:
    prompt = json.load(f)

# éš±è— footer èˆ‡ menu
st.markdown("""
<style>
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# å´é‚Šé¸å–®
with st.sidebar:
    selected = option_menu(
        menu_title="Menu",
        options=["Record", "Upload", "Transcribe", "Summary", "Q&A"],
        icons=["mic", "upload", "book", "file-text", "chat-dots"],
        default_index=0
    )

# session state
if 'transcribe_text' not in st.session_state:
    st.session_state.transcribe_text = None
if 'summary' not in st.session_state:
    st.session_state.summary = None

# è™•ç†å¤§æª”æ¡ˆï¼ˆåˆ‡ç‰‡ä¸Šå‚³ï¼‰
def transcribe_large_audio(audio, chunk_length=20*60*1000):
    full_transcript = ""

    chunks = math.ceil(len(audio) / chunk_length)

    for i in range(chunks):
        chunk = audio[i*chunk_length:(i+1)*chunk_length]
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            chunk.export(tmp.name, format="mp3")
            with open(tmp.name, "rb") as f:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=f,
                    response_format="text"
                )
                full_transcript += transcript + "\n"
            os.unlink(tmp.name)

    return full_transcript.strip()

# summarize
def summarize_text(text):
    messages = prompt + [{"role": "user", "content": text}]
    res = client.chat.completions.create(
        model="gpt-4",
        messages=messages
    )
    return res.choices[0].message.content

# Record é é¢
if selected == "Record":
    st.title("ğŸ¤ å³æ™‚éŒ„éŸ³ç³»çµ±")
    audio_file = st.audio_input("é»æ“Šä¸‹æ–¹æŒ‰éˆ•éŒ„éŸ³", key="recorder")
    AUDIO = audio_file

    if audio_file is not None:
        audio_bytes = audio_file.read()
        st.audio(audio_bytes, format="audio/wav")

        b64 = base64.b64encode(audio_bytes).decode()
        href = f'<a href="data:audio/wav;base64,{b64}" download="recording.wav">ğŸ“¥ ä¸‹è¼‰éŒ„éŸ³</a>'
        st.markdown(href, unsafe_allow_html=True)

# Upload é é¢
if selected == "Upload":
    st.title("ğŸ“ ä¸Šå‚³éŸ³æª”")
    uploaded = st.file_uploader("æ”¯æ´ MP3/WAV/MP4", type=["mp3", "wav", "mp4"])
    AUDIO = uploaded

    if uploaded:
        st.audio(uploaded)
        st.success("âœ… ä¸Šå‚³æˆåŠŸï¼Œè«‹å‰å¾€ Transcribe é é¢")

# Transcribe é é¢
if selected == "Transcribe":
    st.title("ğŸ“ é€å­—ç¨¿")
    if 'audio_path' not in st.session_state:
        st.warning("è«‹å…ˆéŒ„éŸ³æˆ–ä¸Šå‚³éŸ³æª”")
    else:
        if st.button("ğŸ§ é–‹å§‹è½‰è­¯"):
            with st.spinner("è½‰è­¯ä¸­ï¼Œè«‹ç¨å€™..."):
                transcript = transcribe_large_audio(AUDIO)
                st.session_state.transcribe_text = transcript
                st.success("âœ… è½‰è­¯å®Œæˆï¼")

    if st.session_state.transcribe_text:
        st.text_area("é€å­—ç¨¿å…§å®¹", st.session_state.transcribe_text, height=400)

# Summary é é¢
if selected == "Summary":
    st.title("ğŸ§¾ æ‘˜è¦")
    if st.session_state.transcribe_text is None:
        st.warning("è«‹å…ˆå®Œæˆè½‰è­¯")
    else:
        if st.button("ğŸ§  ç”¢ç”Ÿæ‘˜è¦"):
            with st.spinner("ç”Ÿæˆä¸­..."):
                summary = summarize_text(st.session_state.transcribe_text)
                st.session_state.summary = summary
                st.success("âœ… æ‘˜è¦å®Œæˆï¼")

    if st.session_state.summary:
        st.markdown(st.session_state.summary)

# Q&A é é¢
if selected == "Q&A":
    st.title("â“ å•ç­”ç³»çµ±")
    if st.session_state.transcribe_text is None:
        st.warning("è«‹å…ˆå®Œæˆè½‰è­¯")
    else:
        if "messages" not in st.session_state:
            st.session_state.messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœƒè­°åŠ©ç†ï¼Œæ ¹æ“šé€å­—ç¨¿å›ç­”å•é¡Œã€‚"},
                {"role": "user", "content": st.session_state.transcribe_text}
            ]

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if question := st.chat_input("è«‹è¼¸å…¥å•é¡Œ"):
            st.session_state.messages.append({"role": "user", "content": question})
            with st.chat_message("user"):
                st.markdown(question)

            with st.chat_message("assistant"):
                response = client.chat.completions.create(
                    model="gpt-4",
                    messages=st.session_state.messages
                )
                reply = response.choices[0].message.content
                st.markdown(reply)
                st.session_state.messages.append({"role": "assistant", "content": reply})
